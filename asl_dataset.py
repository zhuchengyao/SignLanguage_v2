import os
import json
from typing import List, Optional, Sequence, Tuple

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader


class ASLPoseDataset(Dataset):
    """ASL Pose æ•°æ®é›† (æ”¯æŒå¤–éƒ¨å‡å€¼/æ–¹å·®)

    æ•°æ®æ ¹ç›®å½•ç»“æ„::
        data_root/
            train/  {sample_id}/text.txt & pose.json
            dev/    ...
            test/   ...

    `pose.json` æ¯å¸§å­—æ®µ::
        {
          "poses": [
              {"pose_keypoints_2d": [...24],
               "hand_left_keypoints_2d": [...63],
               "hand_right_keypoints_2d": [...63]},
              ...
          ]
        }

    å‚æ•°
    ------
    split : str                'train' | 'dev' | 'test'
    max_samples : int | None    ä»…åŠ è½½å‰ N ä¸ªæ ·æœ¬ (è°ƒè¯•ç”¨)
    pose_normalize : bool       æ˜¯å¦æ ¹æ®å½“å‰ split é‡æ–°è®¡ç®—å‡å€¼/æ–¹å·®
    extern_mean/std : Sequence  è‹¥ç»™å®š, ç›´æ¥ç”¨å¤–éƒ¨ Î¼/Ïƒ åšå½’ä¸€åŒ–, ä¸å†è‡ªåŠ¨è®¡ç®—
    clip_len : int              å›ºå®šå¸§é•¿, ä¸è¶³ç”¨æœ€åä¸€å¸§è¡¥é½
    pose_clip_range : float     å½’ä¸€åŒ–åæˆªæ–­åŒºé—´ [-k, k]
    """

    def __init__(
        self,
        data_root: str,
        split: str = "train",
        max_samples: Optional[int] = None,
        pose_normalize: bool = True,
        pose_clip_range: float = 3.0,
        clip_len: int = 32,
        extern_mean: Optional[Sequence[float]] = None,
        extern_std: Optional[Sequence[float]] = None,
    ) -> None:
        super().__init__()
        self.data_root = data_root
        self.split = split
        self.pose_normalize = pose_normalize
        self.pose_clip_range = pose_clip_range
        self.clip_len = clip_len

        # å°†å¤–éƒ¨ Î¼/Ïƒ å­˜æˆ ndarray æ–¹ä¾¿å¹¿æ’­
        self.extern_mean = None if extern_mean is None else np.asarray(extern_mean)
        self.extern_std = None if extern_std is None else np.asarray(extern_std)

        self.texts: List[str] = []
        self.poses: List[List[List[float]]] = []  # (T, 150)

        self._load_data(max_samples)

        # å½’ä¸€åŒ–ç­–ç•¥
        if self.extern_mean is not None and self.extern_std is not None:
            self.pose_mean = self.extern_mean
            self.pose_std = self.extern_std
            self._apply_norm("å¤–éƒ¨ Î¼/Ïƒ")
        elif self.pose_normalize and len(self.poses):
            self._compute_and_apply_norm()

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------
    def _load_data(self, max_samples: Optional[int]) -> None:
        split_dir = os.path.join(self.data_root, self.split)
        if not os.path.isdir(split_dir):
            raise FileNotFoundError(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {split_dir}")

        sample_dirs = sorted(
            d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))
        )
        print(f"ğŸ” {self.split}: å‘ç° {len(sample_dirs)} ä¸ªæ ·æœ¬")

        for d in sample_dirs:
            sample_path = os.path.join(split_dir, d)
            text_file = os.path.join(sample_path, "text.txt")
            pose_file = os.path.join(sample_path, "pose.json")
            if not (os.path.exists(text_file) and os.path.exists(pose_file)):
                continue

            try:
                with open(text_file, "r", encoding="utf-8") as f:
                    text = f.read().strip()
                with open(pose_file, "r", encoding="utf-8") as f:
                    js = json.load(f)
            except Exception:
                continue  # skip corrupt

            frames = js.get("poses", [])
            if not frames:
                continue

            seq: List[List[float]] = []
            for fr in frames[: self.clip_len]:
                pose = (
                    fr["pose_keypoints_2d"]
                    + fr["hand_right_keypoints_2d"]
                    + fr["hand_left_keypoints_2d"]
                )
                if len(pose) == 150:
                    seq.append(pose)
            if not seq:
                continue

            while len(seq) < self.clip_len:
                seq.append(seq[-1].copy())  # pad last frame
            self.texts.append(text)
            self.poses.append(seq)

            if max_samples and len(self.texts) >= max_samples:
                break
        print(f"âœ… {self.split}: æˆåŠŸåŠ è½½ {len(self.texts)} ä¸ªæ ·æœ¬")

    def _compute_and_apply_norm(self) -> None:
        all_frames = np.array(self.poses).reshape(-1, 150)  # (N,150)
        self.pose_mean = all_frames.mean(0)
        self.pose_std = all_frames.std(0)
        self._apply_norm("è‡ªåŠ¨ Î¼/Ïƒ")
        print(f"  æ•°æ®å½’ä¸€åŒ– (è‡ªåŠ¨): Î¼â‰ˆ{self.pose_mean.mean():.3f}, Ïƒâ‰ˆ{self.pose_std.mean():.3f}")

    def _apply_norm(self, tag: str) -> None:
        normed = []
        for seq in self.poses:
            arr = np.asarray(seq)
            arr = (arr - self.pose_mean) / (self.pose_std + 1e-8)
            arr = np.clip(arr, -self.pose_clip_range, self.pose_clip_range)
            normed.append(arr.tolist())
        self.poses = normed
        print(f"  æ•°æ®å½’ä¸€åŒ– ({tag}): å‡å€¼=0, æ ‡å‡†å·®=1")

    # ------------------------------------------------------------------
    # Dataset interface
    # ------------------------------------------------------------------
    def __len__(self) -> int:
        return len(self.texts)

    def __getitem__(self, idx: int) -> Tuple[str, torch.Tensor]:
        text = self.texts[idx]
        pose_seq = torch.tensor(self.poses[idx], dtype=torch.float32)  # (T,150)
        return text, pose_seq


# ----------------------------------------------------------------------
# Convenience loader factory
# ----------------------------------------------------------------------

def create_data_loaders(config):
    """æ„å»º train / dev / test çš„ DataLoader, ä¿è¯ Î¼/Ïƒ ä¸€è‡´"""

    # 1âƒ£  å…ˆåŠ è½½ train, è‡ªåŠ¨å½’ä¸€åŒ–
    train_set = ASLPoseDataset(
        data_root=config.data_root,
        split="train",
        pose_normalize=config.pose_normalize,
        pose_clip_range=config.pose_clip_range,
        clip_len=config.clip_len,
    )

    # 2âƒ£  dev / test å¤ç”¨ train çš„ Î¼/Ïƒ
    dev_set = ASLPoseDataset(
        data_root=config.data_root,
        split="dev",
        pose_normalize=False,            # å…³é—­å†…éƒ¨å‡å€¼è®¡ç®—
        pose_clip_range=config.pose_clip_range,
        clip_len=config.clip_len,
        extern_mean=train_set.pose_mean,
        extern_std=train_set.pose_std,
    )

    test_set = ASLPoseDataset(
        data_root=config.data_root,
        split="test",
        pose_normalize=False,
        pose_clip_range=config.pose_clip_range,
        clip_len=config.clip_len,
        extern_mean=train_set.pose_mean,
        extern_std=train_set.pose_std,
    )

    loader_kwargs = dict(
        batch_size=config.batch_size,
        num_workers=0,      # Win / ç®€åŒ–
        pin_memory=True,
    )

    train_loader = DataLoader(train_set, shuffle=True, drop_last=True, **loader_kwargs)
    dev_loader   = DataLoader(dev_set,   shuffle=False, drop_last=False, **loader_kwargs)
    test_loader  = DataLoader(test_set,  shuffle=False, drop_last=False, **loader_kwargs)

    return train_loader, dev_loader, test_loader
