#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ASL ç»Ÿä¸€å¯è§†åŒ–å·¥å…·ï¼ˆé€‚é…â€œæ–¹æ¡ˆ Bâ€æ—¶åºæ‰©æ•£æ¨¡åž‹ï¼‰
åŠŸèƒ½1: å¯è§†åŒ–çœŸå®žæ•°æ®é›†ä¸­çš„ ASL æ ·ä¾‹
åŠŸèƒ½2: ä½¿ç”¨ checkpoints/best.pth æŽ¨ç†ç”Ÿæˆ ASL åŠ¨ç”»å¹¶å¯è§†åŒ–
"""

import os, json
from datetime import datetime
from typing import List

import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ¨¡åž‹ & é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
from model import TextToPoseDiffusion
from config import ModelConfig           # â¬…ï¸ NEW

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.sans-serif"] = ["SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


class ASLVisualizer:
    """ç»Ÿä¸€å¯è§†åŒ–å·¥å…·ï¼Œå…¼å®¹ä¼˜åŒ–åŽæ¨¡åž‹ã€‚"""

    BODY_JOINTS = 8
    HAND_JOINTS = 21

    def __init__(self, checkpoint_path: str = "./checkpoints/best.pth"):
        self.checkpoint_path = checkpoint_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model: TextToPoseDiffusion | None = None
        self.model_cfg: ModelConfig | None = None        # â¬…ï¸ NEW

        # â€”â€” è¿žæŽ¥æ‹“æ‰‘ â€”â€”ï¼ˆä¸Žè®­ç»ƒæ•°æ®ä¸€è‡´ï¼‰
        self.pose_connections = [
            (0, 1),
            (1, 2), (2, 3), (3, 4),
            (1, 5), (5, 6), (6, 7),
        ]
        self.hand_connections = [
            (0, 1), (1, 2), (2, 3), (3, 4),
            (0, 5), (5, 6), (6, 7), (7, 8),
            (0, 9), (9, 10), (10, 11), (11, 12),
            (0, 13), (13, 14), (14, 15), (15, 16),
            (0, 17), (17, 18), (18, 19), (19, 20),
        ]

        print("ðŸš€ ASL å¯è§†åŒ–å·¥å…·åˆå§‹åŒ–å®Œæ¯• (Optimised Model)")
        print("ðŸ“± ä½¿ç”¨è®¾å¤‡:", self.device)

    # ------------------------------------------------------------------
    # åŠŸèƒ½ 1: å¯è§†åŒ–çœŸå®žæ•°æ®é›†æ ·ä¾‹
    # ------------------------------------------------------------------
    def _load_real_sample(self, folder: str):
        txt_path = os.path.join(folder, "text.txt")
        pose_path = os.path.join(folder, "pose.json")
        if not (os.path.exists(txt_path) and os.path.exists(pose_path)):
            raise FileNotFoundError("text.txt æˆ– pose.json ç¼ºå¤±")
        with open(txt_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        with open(pose_path, "r", encoding="utf-8") as f:
            poses = json.load(f)["poses"]
        return text, poses

    def visualize_real_sample(self, folder: str,
                              max_frames: int = 32, interval: int = 250):
        text, poses = self._load_real_sample(folder)
        poses = poses[:max_frames]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe = "".join(c for c in text if c.isalnum() or c in ("_", " ")).replace(" ", "_")
        out_path = f"real_{safe}_{timestamp}.gif"
        self._create_animation(poses, text, out_path, interval, "Real Data")

    # ------------------------------------------------------------------
    # åŠŸèƒ½ 2: æŽ¨ç†å¹¶å¯è§†åŒ–
    # ------------------------------------------------------------------
    def _load_model(self):
        if self.model is not None:
            return self.model

        print("ðŸ“¦ æ­£åœ¨åŠ è½½æ¨¡åž‹â€¦")
        # â”€â”€ 1) è¯»å– checkpoint
        ckpt = None
        if os.path.exists(self.checkpoint_path):
            ckpt = torch.load(self.checkpoint_path, map_location=self.device)
            print(f"ðŸ”‘ ä»Ž {self.checkpoint_path} è¯»å–æƒé‡")

        # â”€â”€ 2) ç¡®å®š ModelConfig
        if ckpt and "model_cfg" in ckpt:
            self.model_cfg = ModelConfig(**ckpt["model_cfg"])
            print("ðŸ“ ä½¿ç”¨ checkpoint å†…ä¿å­˜çš„ model_cfg")
        else:
            self.model_cfg = ModelConfig()
            print("ðŸ“ ä½¿ç”¨é»˜è®¤ ModelConfig()")

        # â”€â”€ 3) æž„å»ºæ¨¡åž‹å¹¶åŠ è½½æƒé‡
        self.model = TextToPoseDiffusion(self.model_cfg).to(self.device)
        if ckpt:
            state_dict = ckpt.get("model_state_dict", ckpt)
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            if missing:
                print("âš ï¸  ç¼ºå°‘å‚æ•°:", missing)
            if unexpected:
                print("âš ï¸  å¤šä½™å‚æ•°:", unexpected)
        self.model.eval()
        print("âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
        return self.model

    @torch.no_grad()
    def infer_and_visualize(self, text: str,
                            steps: int = 300, interval: int = 100):
        model = self._load_model()
        print(f"ðŸ¤– æŽ¨ç† '{text}' (steps={steps})â€¦")
        seq = model.sample([text], num_steps=steps).cpu().numpy()[0]   # (T, pose_dim)

        pose_dim = seq.shape[1]
        assert pose_dim == self.BODY_JOINTS * 3 + 2 * self.HAND_JOINTS * 3, \
            f"pose_dim ({pose_dim}) ä¸Ž 50Ã—3 ä¸ç¬¦"

        body_end = self.BODY_JOINTS * 3
        lh_end   = body_end + self.HAND_JOINTS * 3

        body = seq[:, :body_end]
        lh   = seq[:, body_end:lh_end]
        rh   = seq[:, lh_end:]

        T = seq.shape[0]
        poses: List[dict] = []
        for i in range(T):
            frame = {
                "pose_keypoints_2d":      body[i].tolist(),
                "hand_left_keypoints_2d": lh[i].tolist(),
                "hand_right_keypoints_2d": rh[i].tolist(),
            }
            poses.append(frame)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe = "".join(c for c in text if c.isalnum() or c in ("_", " ")).replace(" ", "_")
        out_path = f"infer_{safe}_{timestamp}.gif"
        self._create_animation(poses, text, out_path, interval, "Inference")

    # ------------------------------------------------------------------
    # ç»˜åˆ¶ / åŠ¨ç”» é€šç”¨æ–¹æ³•
    # ------------------------------------------------------------------
    @staticmethod
    def _np_points(kpts):
        return np.array(kpts).reshape(-1, 3)

    def _draw_edges(self, ax, pts, conns, p_col, l_col, p_size, l_w):
        valid = (np.abs(pts[:, 0]) > 1e-6) | (np.abs(pts[:, 1]) > 1e-6)
        if valid.any():
            ax.scatter(pts[valid, 0], -pts[valid, 1],
                       c=p_col, s=p_size, alpha=0.85)
        for a, b in conns:
            if a < len(pts) and b < len(pts) and valid[a] and valid[b]:
                ax.plot([pts[a, 0], pts[b, 0]],
                        [-pts[a, 1], -pts[b, 1]],
                        color=l_col, linewidth=l_w, alpha=0.8)

    def _create_animation(self, poses, text, out_path,
                          interval, tag):
        print(f"ðŸŽ¬ ç”Ÿæˆ {tag} åŠ¨ç”»ï¼Œå…± {len(poses)} å¸§ â†’ {out_path}")
        fig, ax = plt.subplots(figsize=(7, 7))

        # è®¡ç®—å¯è§†åŒºåŸŸ
        coords = []
        for fr in poses:
            for key in ("pose_keypoints_2d",
                        "hand_left_keypoints_2d",
                        "hand_right_keypoints_2d"):
                coords.append(self._np_points(fr[key])[:, :2])
        coords = np.concatenate(coords, axis=0)
        xmin, xmax = coords[:, 0].min(), coords[:, 0].max()
        ymin, ymax = coords[:, 1].min(), coords[:, 1].max()
        margin = 0.1 * max(xmax - xmin, ymax - ymin, 1)
        xlim = (xmin - margin, xmax + margin)
        ylim = (-(ymax + margin), -(ymin - margin))

        def _update(i):
            ax.clear()
            ax.set_xlim(xlim); ax.set_ylim(ylim); ax.set_aspect("equal")
            ax.grid(True, alpha=0.3)

            fr = poses[i]
            body = self._np_points(fr["pose_keypoints_2d"])
            lh   = self._np_points(fr["hand_left_keypoints_2d"])
            rh   = self._np_points(fr["hand_right_keypoints_2d"])

            self._draw_edges(ax, body, self.pose_connections,
                             "red", "blue", 40, 2)
            self._draw_edges(ax, lh, self.hand_connections,
                             "green", "green", 20, 1.5)
            self._draw_edges(ax, rh, self.hand_connections,
                             "orange", "orange", 20, 1.5)

            ax.set_title(f'{tag}: "{text}" | Frame {i+1}/{len(poses)}')

        ani = animation.FuncAnimation(fig, _update,
                                      frames=len(poses),
                                      interval=interval, blit=False)
        ani.save(out_path, writer="pillow", fps=1000 / interval)
        print("âœ… åŠ¨ç”»ä¿å­˜æˆåŠŸ")
        plt.show()


# ------------------------------------------------------------------
# CLI å…¥å£
# ------------------------------------------------------------------
def main():
    vis = ASLVisualizer()
    while True:
        print("\n=== é€‰æ‹©åŠŸèƒ½ ===\n1) å¯è§†åŒ–çœŸå®žæ•°æ®æ ·ä¾‹\n2) æŽ¨ç†ç”Ÿæˆå¹¶å¯è§†åŒ–\n3) é€€å‡º")
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2/3): ").strip()
        if choice == "1":
            folder = input("æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
            vis.visualize_real_sample(folder) if os.path.isdir(folder) \
                else print("âŒ è·¯å¾„ä¸å­˜åœ¨")
        elif choice == "2":
            text = input("è¾“å…¥è¦ç”Ÿæˆçš„ ASL è¯è¯­: ").strip()
            vis.infer_and_visualize(text) if text else print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
        elif choice == "3":
            print("ðŸ‘‹ å†è§ï¼"); break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()
